{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# List of models including Claude\n",
    "models = [\"qwen2.5:14b\", \"qwen2.5:7b\", \"qwen2.5:3b\", \"qwen2.5:1.5b\", \"mistral-small:latest\", \"mistral-nemo:latest\", \"gemma2:27b\", \"gemma2:9b\", \"gemma2:2b\", \"llama3.1:8b\", \"Claude 3.5 Sonnet\"]  # \"llama3.1:70b\" Replace with your actual model names\n",
    "#models = [\"qwen2.5:3b\", \"mistral-nemo:latest\", \"gemma2:2b\", \"llama3.1:8b\", \"Claude 3.5 Sonnet\"]\n",
    "#models = [\"llama3.1:8b\"]\n",
    "\n",
    "anthropic_api_key = \"\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key\n",
    "\n",
    "# Prompt\n",
    "confidence_prompt = \"On a scale of 1 to 100, where 1 represents absolutely no confidence and 100 represents complete certainty, how would you rate your confidence in your ability to correctly answer a series of multiple choice questions drawn from a set designed to test the knowledge of final year medical students? The format is a clinical vignette followed by a set of multiple choice answers, of which one is correct. The clinical questions do not relate to actual patients, and are not designed to be used to guide clinical practice. Clinical examination is not required, and there is no disadvantage in not being able to physically see a patient. Please provide your numerical rating followed by a brief explanation of why you chose that rating.\"\n",
    "\n",
    "# Load questions and answers\n",
    "def load_questions(filename='combined_qa.txt', limit=None):\n",
    "    print(f\"Loading questions from {filename}...\")\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    questions = []\n",
    "    answers = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('Answer: ')\n",
    "        if len(parts) == 2:\n",
    "            questions.append(parts[0].strip())\n",
    "            answers.append(parts[1].strip())\n",
    "    \n",
    "    if limit:\n",
    "        questions = questions[:limit]\n",
    "        answers = answers[:limit]\n",
    "    \n",
    "    print(f\"Loaded {len(questions)} questions.\")\n",
    "    return questions, answers\n",
    "\n",
    "def bootstrap_sample(questions, answers, sample_size=10):\n",
    "    indices = random.choices(range(len(questions)), k=sample_size)\n",
    "    return [questions[i] for i in indices], [answers[i] for i in indices]\n",
    "\n",
    "def cap_confidence(value):\n",
    "    \"\"\"Cap the confidence value at 100.\"\"\"\n",
    "    return min(value, 100) if value is not None else None\n",
    "\n",
    "def evaluate_model(model, questions, correct_answers, confidence_prompt):\n",
    "    print(f\"\\nEvaluating model: {model}\")\n",
    "    if not questions or not correct_answers:\n",
    "        print(f\"Warning: No questions or answers available for evaluation of model {model}\")\n",
    "        return None, 0\n",
    "    \n",
    "    print(\"Querying model for confidence rating...\")\n",
    "    if model == \"Claude 3.5 Sonnet\":\n",
    "        confidence_response = query_claude(confidence_prompt)\n",
    "    else:\n",
    "        confidence_response = query_ollama(model, confidence_prompt)\n",
    "    \n",
    "    confidence_rating = extract_number(confidence_response)\n",
    "    confidence_rating = cap_confidence(confidence_rating)\n",
    "    print(f\"Confidence rating: {confidence_rating}\")\n",
    "    \n",
    "    correct_count = 0\n",
    "    print(\"Evaluating questions...\")\n",
    "    for i, (question, correct_answer) in enumerate(zip(questions, correct_answers), 1):\n",
    "        print(f\"  Processing question {i}/{len(questions)}...\")\n",
    "        if model == \"Claude 3.5 Sonnet\":\n",
    "            response = query_claude(question)\n",
    "        else:\n",
    "            response = query_ollama(model, question)\n",
    "        \n",
    "        answer = extract_answer(response)\n",
    "        if answer and answer.upper() == correct_answer.upper():\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = (correct_count / len(questions)) * 100 if questions else 0\n",
    "    print(f\"Evaluation complete. Accuracy: {accuracy:.2f}%\")\n",
    "    return confidence_rating, accuracy\n",
    "\n",
    "def run_bootstrap_evaluation(models, n_iterations=4, sample_size=10):\n",
    "    print(\"Starting bootstrap evaluation...\")\n",
    "    all_questions, all_answers = load_questions()\n",
    "    if not all_questions or not all_answers:\n",
    "        print(\"Error: No questions or answers available for evaluation.\")\n",
    "        return {}, {}\n",
    "    \n",
    "    results = {model: {'confidence': [], 'accuracy': []} for model in models}\n",
    "    individual_results = {model: [] for model in models}\n",
    "    \n",
    "    actual_sample_size = min(sample_size, len(all_questions))\n",
    "    print(f\"Using sample size of {actual_sample_size}\")\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        print(f\"\\nStarting bootstrap iteration {iteration + 1}/{n_iterations}\")\n",
    "        questions, answers = bootstrap_sample(all_questions, all_answers, actual_sample_size)\n",
    "        \n",
    "        for model in models:\n",
    "            print(f\"\\nProcessing model: {model}\")\n",
    "            confidence, accuracy = evaluate_model(model, questions, answers, confidence_prompt)\n",
    "            if confidence is not None:\n",
    "                results[model]['confidence'].append(confidence)\n",
    "                results[model]['accuracy'].append(accuracy)\n",
    "                individual_results[model].append({\n",
    "                    'iteration': iteration + 1,\n",
    "                    'confidence': confidence,\n",
    "                    'accuracy': accuracy\n",
    "                })\n",
    "                print(f\"Results for {model}: Confidence = {confidence}, Accuracy = {accuracy:.2f}%\")\n",
    "    \n",
    "    return results, individual_results\n",
    "\n",
    "# Function to interact with Ollama model\n",
    "def query_ollama(model, prompt, temperature=0.4, timeout=120):\n",
    "    print(f\"Querying Ollama model: {model}\")\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=data, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying Ollama model {model}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to interact with Claude 3.5 Sonnet\n",
    "def query_claude(prompt):\n",
    "    print(\"Querying Claude 3.5 Sonnet\")\n",
    "    client = Anthropic()\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=1000,\n",
    "            temperature=0.4,\n",
    "            system=\"You are a helpful AI assistant.\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Claude 3.5 Sonnet: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract number from response\n",
    "def extract_number(response):\n",
    "    if response:\n",
    "        match = re.search(r'\\b(\\d+)\\b', response)\n",
    "        return int(match.group(1)) if match else None\n",
    "    return None\n",
    "\n",
    "# Function to extract answer from response\n",
    "def extract_answer(response):\n",
    "    if response:\n",
    "        match = re.search(r'\\b([A-E])\\b', response)\n",
    "        return match.group(1) if match else None\n",
    "    return None\n",
    "\n",
    "print(\"Starting evaluation process...\")\n",
    "# Run bootstrap evaluation\n",
    "bootstrap_results, individual_results = run_bootstrap_evaluation(models, n_iterations=4, sample_size=50)\n",
    "\n",
    "if not bootstrap_results:\n",
    "    print(\"Evaluation could not be completed due to lack of questions or answers.\")\n",
    "else:\n",
    "    print(\"\\nCalculating summary results...\")\n",
    "    # Calculate mean and confidence intervals\n",
    "    summary_results = {}\n",
    "    for model, data in bootstrap_results.items():\n",
    "        if data['confidence'] and data['accuracy']:\n",
    "            confidence_mean = np.mean(data['confidence'])\n",
    "            confidence_ci = np.percentile(data['confidence'], [2.5, 97.5])\n",
    "            accuracy_mean = np.mean(data['accuracy'])\n",
    "            accuracy_ci = np.percentile(data['accuracy'], [2.5, 97.5])\n",
    "            \n",
    "            summary_results[model] = {\n",
    "                'confidence_mean': confidence_mean,\n",
    "                'confidence_ci': confidence_ci.tolist(),\n",
    "                'accuracy_mean': accuracy_mean,\n",
    "                'accuracy_ci': accuracy_ci.tolist()\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: No valid data for model {model}\")\n",
    "\n",
    "    # Save summary results\n",
    "    print(\"Saving summary results to bootstrap_summary_results.json...\")\n",
    "    with open(\"bootstrap_summary_results.json\", \"w\") as f:\n",
    "        json.dump(summary_results, f, indent=2)\n",
    "\n",
    "    # Save individual results\n",
    "    print(\"Saving individual results to bootstrap_individual_results.json...\")\n",
    "    with open(\"bootstrap_individual_results.json\", \"w\") as f:\n",
    "        json.dump(individual_results, f, indent=2)\n",
    "\n",
    "    print(\"Bootstrap results have been saved to bootstrap_summary_results.json and bootstrap_individual_results.json\")\n",
    "\n",
    "    # Print mean accuracies and number of questions used\n",
    "    n_questions = len(load_questions()[0])\n",
    "    print(f\"\\nNumber of questions available: {n_questions}\")\n",
    "    print(f\"Sample size used: {min(50, n_questions)}\")\n",
    "    for model, data in summary_results.items():\n",
    "        print(f\"{model}: Mean accuracy = {data['accuracy_mean']:.2f}%, Confidence = {data['confidence_mean']:.2f}\")\n",
    "\n",
    "print(\"\\nEvaluation process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the bootstrap results\n",
    "with open(\"bootstrap_results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Prepare data for plotting\n",
    "models = list(results.keys())\n",
    "confidence_means = [results[model]['confidence_mean'] for model in models]\n",
    "confidence_errors = [\n",
    "    [results[model]['confidence_mean'] - results[model]['confidence_ci'][0],\n",
    "     results[model]['confidence_ci'][1] - results[model]['confidence_mean']]\n",
    "    for model in models\n",
    "]\n",
    "accuracy_means = [results[model]['accuracy_mean'] for model in models]\n",
    "accuracy_errors = [\n",
    "    [results[model]['accuracy_mean'] - results[model]['accuracy_ci'][0],\n",
    "     results[model]['accuracy_ci'][1] - results[model]['accuracy_mean']]\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.errorbar(confidence_means, accuracy_means, xerr=np.array(confidence_errors).T, \n",
    "             yerr=np.array(accuracy_errors).T, fmt='o', capsize=5, ecolor='gray', markersize=8)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    plt.annotate(model, (confidence_means[i], accuracy_means[i]), xytext=(5, 5), \n",
    "                 textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.title(\"LLM Confidence vs Actual Performance (with 95% CIs)\", fontsize=16)\n",
    "plt.xlabel(\"Confidence Rating\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "\n",
    "# Add a diagonal line representing perfect calibration\n",
    "min_val = min(min(confidence_means), min(accuracy_means))\n",
    "max_val = max(max(confidence_means), max(accuracy_means))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='Perfect Calibration')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"bootstrap_confidence_vs_performance.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Plot has been saved as 'bootstrap_confidence_vs_performance.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def plot_performance_vs_confidence(data_file='bootstrap_results_run2.json'):\n",
    "    # Load data\n",
    "    with open(data_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Prepare data for plotting, excluding mistral-nemo\n",
    "    models = [model for model in data.keys() if model != \"mistral-nemo:latest\"]\n",
    "    confidences = [data[model]['confidence_mean'] for model in models]\n",
    "    confidence_errors = [\n",
    "        [data[model]['confidence_mean'] - data[model]['confidence_ci'][0],\n",
    "         data[model]['confidence_ci'][1] - data[model]['confidence_mean']]\n",
    "        for model in models\n",
    "    ]\n",
    "    performances = [data[model]['accuracy_mean'] for model in models]\n",
    "    performance_errors = [\n",
    "        [data[model]['accuracy_mean'] - data[model]['accuracy_ci'][0],\n",
    "         data[model]['accuracy_ci'][1] - data[model]['accuracy_mean']]\n",
    "        for model in models\n",
    "    ]\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot points with error bars (95% confidence intervals)\n",
    "    ax.errorbar(confidences, performances, \n",
    "                xerr=np.array(confidence_errors).T, \n",
    "                yerr=np.array(performance_errors).T, \n",
    "                fmt='o', capsize=5, capthick=1, ecolor='gray', alpha=0.5)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, model in enumerate(models):\n",
    "        ax.annotate(model, (confidences[i], performances[i]), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), \n",
    "                    ha='center', fontsize=8, rotation=45)\n",
    "    \n",
    "    # Calculate and plot linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(confidences, performances)\n",
    "    line = slope * np.array([min(confidences), max(confidences)]) + intercept\n",
    "    ax.plot([min(confidences), max(confidences)], line, 'g-', label=f'Linear regression (r={r_value:.2f})')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Confidence (Mean)')\n",
    "    ax.set_ylabel('Performance (Mean Accuracy %)')\n",
    "    ax.set_title('Performance vs Confidence with 95% Confidence Intervals and Linear Regression')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set axis limits\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    # Add diagonal line y=x\n",
    "    ax.plot([0, 100], [0, 100], 'r--', alpha=0.5, label='y=x')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('performance_vs_confidence_with_ci_and_regression.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Performance vs Confidence plot with 95% confidence intervals and linear regression has been saved as performance_vs_confidence_with_ci_and_regression.png\")\n",
    "    print(f\"Linear regression results:\")\n",
    "    print(f\"Slope: {slope:.4f}\")\n",
    "    print(f\"Intercept: {intercept:.4f}\")\n",
    "    print(f\"R-squared: {r_value**2:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Run the function\n",
    "plot_performance_vs_confidence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def get_parameter_sizes():\n",
    "    return {\n",
    "        \"qwen2.5:14b\": 14,\n",
    "        \"qwen2.5:7b\": 7,\n",
    "        \"qwen2.5:3b\": 3,\n",
    "        \"qwen2.5:1.5b\": 1.5,\n",
    "        \"mistral-small:latest\": 7,\n",
    "        \"mistral-nemo:latest\": 7,\n",
    "        \"gemma2:27b\": 27,\n",
    "        \"gemma2:9b\": 9,\n",
    "        \"gemma2:2b\": 2,\n",
    "        \"llama3.1:8b\": 8,\n",
    "        \"Claude 3.5 Sonnet\": 175  # Estimated\n",
    "    }\n",
    "\n",
    "def plot_performance_vs_confidence(data, output_file):\n",
    "    models = [model for model in data.keys() if model != \"mistral-nemo:latest\"]\n",
    "    confidences = [data[model]['confidence_mean'] for model in models]\n",
    "    confidence_errors = [\n",
    "        [data[model]['confidence_mean'] - data[model]['confidence_ci'][0],\n",
    "         data[model]['confidence_ci'][1] - data[model]['confidence_mean']]\n",
    "        for model in models\n",
    "    ]\n",
    "    performances = [data[model]['accuracy_mean'] for model in models]\n",
    "    performance_errors = [\n",
    "        [data[model]['accuracy_mean'] - data[model]['accuracy_ci'][0],\n",
    "         data[model]['accuracy_ci'][1] - data[model]['accuracy_mean']]\n",
    "        for model in models\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    ax.errorbar(confidences, performances, \n",
    "                xerr=np.array(confidence_errors).T, \n",
    "                yerr=np.array(performance_errors).T, \n",
    "                fmt='o', capsize=5, capthick=1, ecolor='gray', alpha=0.5)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        ax.annotate(model, (confidences[i], performances[i]), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), \n",
    "                    ha='center', fontsize=8, rotation=45)\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(confidences, performances)\n",
    "    line = slope * np.array([min(confidences), max(confidences)]) + intercept\n",
    "    ax.plot([min(confidences), max(confidences)], line, 'g-', label=f'Linear regression (r={r_value:.2f})')\n",
    "    \n",
    "    ax.set_xlabel('Confidence (Mean)')\n",
    "    ax.set_ylabel('Performance (Mean Accuracy %)')\n",
    "    ax.set_title('Performance vs Confidence with 95% Confidence Intervals and Linear Regression')\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.plot([0, 100], [0, 100], 'r--', alpha=0.5, label='y=x')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Performance vs Confidence plot saved as {output_file}\")\n",
    "    print(f\"Linear regression results:\")\n",
    "    print(f\"Slope: {slope:.4f}\")\n",
    "    print(f\"Intercept: {intercept:.4f}\")\n",
    "    print(f\"R-squared: {r_value**2:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "def plot_vs_parameters(data, y_key, y_label, output_file):\n",
    "    parameter_sizes = get_parameter_sizes()\n",
    "    models = [model for model in data.keys() if model != \"mistral-nemo:latest\"]\n",
    "    parameters = [parameter_sizes[model] for model in models]\n",
    "    y_values = [data[model][f'{y_key}_mean'] for model in models]\n",
    "    y_errors = [\n",
    "        [data[model][f'{y_key}_mean'] - data[model][f'{y_key}_ci'][0],\n",
    "         data[model][f'{y_key}_ci'][1] - data[model][f'{y_key}_mean']]\n",
    "        for model in models\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    ax.errorbar(parameters, y_values, \n",
    "                yerr=np.array(y_errors).T, \n",
    "                fmt='o', capsize=5, capthick=1, ecolor='gray', alpha=0.5)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        ax.annotate(model, (parameters[i], y_values[i]), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), \n",
    "                    ha='center', fontsize=8, rotation=45)\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(np.log10(parameters), y_values)\n",
    "    x_line = np.logspace(np.log10(min(parameters)), np.log10(max(parameters)), 100)\n",
    "    y_line = slope * np.log10(x_line) + intercept\n",
    "    ax.plot(x_line, y_line, 'g-', label=f'Log-linear regression (r={r_value:.2f})')\n",
    "    \n",
    "    ax.set_xlabel('Parameter Count (Billions)')\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(f'{y_label} vs Parameter Count with 95% Confidence Intervals')\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(1, 200)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"{y_label} vs Parameter Count plot saved as {output_file}\")\n",
    "    print(f\"Log-linear regression results:\")\n",
    "    print(f\"Slope: {slope:.4f}\")\n",
    "    print(f\"Intercept: {intercept:.4f}\")\n",
    "    print(f\"R-squared: {r_value**2:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "def main(data_file='bootstrap_results_run2.json'):\n",
    "    with open(data_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    plot_performance_vs_confidence(data, 'performance_vs_confidence.png')\n",
    "    plot_vs_parameters(data, 'accuracy', 'Performance (Mean Accuracy %)', 'performance_vs_parameters.png')\n",
    "    plot_vs_parameters(data, 'confidence', 'Confidence (Mean)', 'confidence_vs_parameters.png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def load_results(filename='bootstrap_results.json'):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def add_trend_line(ax, x, y):\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(x, p(x), \"r--\", alpha=0.8)\n",
    "\n",
    "def plot_results(results):\n",
    "    models = list(results.keys())\n",
    "    confidences = [results[model]['confidence_mean'] for model in models]\n",
    "    accuracies = [results[model]['accuracy_mean'] for model in models]\n",
    "    \n",
    "    # Estimate parameter sizes (you may need to adjust these or provide actual values)\n",
    "    parameter_sizes = {\n",
    "        \"qwen2.5:14b\": 14, \"qwen2.5:7b\": 7, \"qwen2.5:3b\": 3, \"qwen2.5:1.5b\": 1.5,\n",
    "        \"mistral-small:latest\": 7, \"mistral-nemo:latest\": 8,\n",
    "        \"gemma2:27b\": 27, \"gemma2:9b\": 9, \"gemma2:2b\": 2,\n",
    "        \"llama3.1:8b\": 8, \"Claude 3.5 Sonnet\": 175  # Estimate for Claude\n",
    "    }\n",
    "    sizes = [parameter_sizes.get(model, 1) for model in models]  # Default to 1 if unknown\n",
    "    \n",
    "    # 1. Confidence vs Performance\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.scatter(confidences, accuracies)\n",
    "    for i, model in enumerate(models):\n",
    "        ax.annotate(model, (confidences[i], accuracies[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "    add_trend_line(ax, confidences, accuracies)\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('Confidence vs Performance')\n",
    "    plt.savefig('confidence_vs_performance.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Confidence vs Parameter Size\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.scatter(sizes, confidences)\n",
    "    for i, model in enumerate(models):\n",
    "        ax.annotate(model, (sizes[i], confidences[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "    add_trend_line(ax, np.log10(sizes), confidences)  # Use log of sizes for trend line\n",
    "    ax.set_xlabel('Parameter Size (billions)')\n",
    "    ax.set_ylabel('Confidence')\n",
    "    ax.set_title('Confidence vs Parameter Size')\n",
    "    ax.set_xscale('log')\n",
    "    plt.savefig('confidence_vs_parameter_size.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Performance vs Parameter Size\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.scatter(sizes, accuracies)\n",
    "    for i, model in enumerate(models):\n",
    "        ax.annotate(model, (sizes[i], accuracies[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "    add_trend_line(ax, np.log10(sizes), accuracies)  # Use log of sizes for trend line\n",
    "    ax.set_xlabel('Parameter Size (billions)')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('Performance vs Parameter Size')\n",
    "    ax.set_xscale('log')\n",
    "    plt.savefig('performance_vs_parameter_size.png')\n",
    "    plt.close()\n",
    "\n",
    "# Load and plot results\n",
    "results = load_results()\n",
    "plot_results(results)\n",
    "print(\"Plots with trend lines have been saved as PNG files in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "import random\n",
    "\n",
    "def query_confidence_distribution(models, confidence_prompt, n_iterations=20):\n",
    "    results = {model: [] for model in models}\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nQuerying model: {model}\")\n",
    "        for _ in tqdm(range(n_iterations)):\n",
    "            if model == \"Claude 3.5 Sonnet\":\n",
    "                confidence = query_claude(confidence_prompt)\n",
    "            else:\n",
    "                confidence = query_ollama(model, confidence_prompt)\n",
    "            \n",
    "            if confidence is not None:\n",
    "                results[model].append(confidence)\n",
    "            \n",
    "            # Add a small delay to avoid rate limiting\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def query_ollama(model, prompt, temperature=0.7, timeout=120):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=data, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return extract_number(response.json()[\"response\"])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying Ollama model {model}: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_claude(prompt):\n",
    "    client = Anthropic()\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7,\n",
    "            system=\"You are a helpful AI assistant.\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return extract_number(message.content[0].text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Claude 3.5 Sonnet: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_number(response):\n",
    "    if response:\n",
    "        match = re.search(r'\\b(\\d+)\\b', response)\n",
    "        return int(match.group(1)) if match else None\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "models = [\"qwen2.5:14b\", \"qwen2.5:7b\", \"qwen2.5:3b\", \"qwen2.5:1.5b\", \"mistral-small:latest\", \"mistral-nemo:latest\", \"gemma2:27b\", \"gemma2:9b\", \"gemma2:2b\", \"llama3.1:8b\", \"Claude 3.5 Sonnet\"]\n",
    "confidence_prompt = \"On a scale of 1 to 100, where 1 represents absolutely no confidence and 100 represents complete certainty, how would you rate your confidence in your ability to correctly answer a series of multiple choice questions drawn from a set designed to test the knowledge of final year medical students? The format is a clinical vignette followed by a set of multiple choice answers, of which one is correct. The clinical questions do not relate to actual patients, and are not designed to be used to guide clinical practice. Clinical examination is not required, and there is no disadvantage in not being able to physically see a patient. Please provide your numerical rating followed by a brief explanation of why you chose that rating.\"\n",
    "\n",
    "confidence_distribution = query_confidence_distribution(models, confidence_prompt, n_iterations=20)\n",
    "\n",
    "# Save results\n",
    "with open(\"confidence_distribution.json\", \"w\") as f:\n",
    "    json.dump(confidence_distribution, f, indent=2)\n",
    "\n",
    "print(\"Confidence distribution results have been saved to confidence_distribution.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_median_confidence_vs_performance(confidence_file='confidence_distribution.json', performance_file='bootstrap_results.json'):\n",
    "    # Load confidence distribution data\n",
    "    with open(confidence_file, 'r') as f:\n",
    "        confidence_data = json.load(f)\n",
    "    \n",
    "    # Load performance data\n",
    "    with open(performance_file, 'r') as f:\n",
    "        performance_data = json.load(f)\n",
    "    \n",
    "    # Calculate median confidence and IQR for each model\n",
    "    median_confidences = {}\n",
    "    confidence_iqrs = {}\n",
    "    for model, ratings in confidence_data.items():\n",
    "        median_confidences[model] = np.median(ratings)\n",
    "        q1, q3 = np.percentile(ratings, [25, 75])\n",
    "        confidence_iqrs[model] = q3 - q1\n",
    "    \n",
    "    # Extract performance (accuracy) for each model\n",
    "    performances = {model: data['accuracy_mean'] for model, data in performance_data.items()}\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    models = list(set(median_confidences.keys()) & set(performances.keys()))\n",
    "    confidences = [median_confidences[model] for model in models]\n",
    "    accuracies = [performances[model] for model in models]\n",
    "    iqrs = [confidence_iqrs[model] for model in models]\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    scatter = ax.scatter(confidences, accuracies, s=100, alpha=0.6)\n",
    "    \n",
    "    # Add error bars for IQR\n",
    "    ax.errorbar(confidences, accuracies, xerr=np.array(iqrs)/2, fmt='none', ecolor='gray', alpha=0.5)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, model in enumerate(models):\n",
    "        ax.annotate(model, (confidences[i], accuracies[i]), textcoords=\"offset points\", xytext=(0,5), ha='center')\n",
    "    \n",
    "    # Add a trend line\n",
    "    z = np.polyfit(confidences, accuracies, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(confidences, p(confidences), \"r--\", alpha=0.8)\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    correlation = np.corrcoef(confidences, accuracies)[0, 1]\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Median Confidence')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'Median Confidence vs Performance\\nCorrelation: {correlation:.2f}')\n",
    "    \n",
    "    # Add grid lines\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add a legend explaining the error bars\n",
    "    ax.plot([], [], 'k-', label='Confidence IQR')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('median_confidence_vs_performance_with_iqr.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Median Confidence vs Performance plot with IQR has been saved as median_confidence_vs_performance_with_iqr.png\")\n",
    "\n",
    "# Example usage\n",
    "plot_median_confidence_vs_performance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
